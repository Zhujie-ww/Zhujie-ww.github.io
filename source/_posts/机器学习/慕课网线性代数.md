---
tags:
  - 线性代数
categories:
  - 机器学习
top: true
abbrlink: 1000
mathjax: true
cover: true
coverImg: https://cdn.jsdelivr.net/gh/Zhujie-ww/zhujie-ww.github.io/medias/文章图片/0.jpg
img: https://cdn.jsdelivr.net/gh/Zhujie-ww/zhujie-ww.github.io/medias/featureimages/13.jpg
summary: "线性代数在机器学习中至关重要，本文力图以几何视角再探线性代数，祝你好运!"
---



<h1 align = "center">概述</h1>


1、线性代数研究一组数，即向量，
&emsp;&ensp;以及向量的“函数”-矩阵；

> 一组数更能真实模拟与描述我们的三维世界，单变量函数不精确；


2、机器学习学习方法：
&emsp;&emsp;&emsp;&emsp;&emsp;不一定要学透数学再学机器学习，
> 可以先实战、先入门然后有目的的补数学，带着目标学习；

3、机器学习需要：
高数、线代、统计学（最重要）、凸优化

4、花书：深度学习圣经：

&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;[花书链接](http://www.deeplearningbook.org/)

4、一手资源几乎全是英语，建议平时加强英语学习；





<h1 align = "center">向量</h1>


1、向量：

① 起源：表示方向的有向线段，物理中速度、加速度；

② 与起点无关，默认从0,0开始；

③ 向量=点/有向线段；

④ 分类；行、列向量；

⑤ 向量加法：先走a在走b等价于先走b后走a；

//向量（5,2）先向x方向走5步再向y方向走2步；

⑥ 计算机科学常用证明方法：反证法、归纳法；

⑦ 向量内积：三角形借助余弦定理求出,定义为内积;

// 内积几何意义：投影

⑧ 点乘的应用：判断两个向量的相似程度（推荐系统）：

//推荐相似的物品，每一个物品都是高位空间的点（电影价格、导演、评分、主演）

每两个电影有夹角，锐角---相似；垂直：无关；
如果点乘越大的正数：考虑完全重合情形；

<h1 align = "center">矩阵</h1>


1、矩阵的几种理解：

① 矩阵=数表，工资表;

② 矩阵=线性系统（线性方程组），

如经济系统，网络中交通网络、信息网络，电路系统中电阻等式；化学系统中等式问题；

③ 矩阵乘法：行点乘，列组合；

④ 矩阵乘法：相当于向量的函数；

⑤ 矩阵=线性空间

列向量-第一列=基坐标1，第二列=基坐标2，张成新空间；

//表示新的空间中点的新坐标；

⑥ 矩阵=线性变换--恒纵坐标拉伸旋转变换；

可以直接写出翻转旋转的空间坐标（不能表示平移变换，因此引入仿射变换概念）；

⑦ 初等矩阵=初等变换；

2、求解矩阵的逆：

方法：(A|E)初等行变换，

**电算步骤：**
> 高斯（从上到下）-约旦（从下至上）方法；

> 本质：高斯-约旦消元法等价于找到一系列初等矩阵使得：
{% raw%}

$${E_1}{E_2} \ldots {E_k} = A$ $

{% endraw %}
所以：求逆矩阵=求逆变换；

3、矩阵的逆重要性：

假设A可逆直接求出,那么此时当A不变，b有很多个时，此时可以复用,`减少计算量`；

4、矩阵的分解: **LU分解**

① 分解目的：提高计算效率；

② 含义： L：单位下三角-主对角线为1；U：上三角；

③方法：高斯消元；

> 高斯消元本质：
>> 初等变换为一个上三角矩阵；

④时间复杂度：{% raw%}$T = O(\frac{{{n^3}}}{2})${% endraw %}


(即求解L矩阵的次数)


证明：
左下方：
第一列下n-1个数字，每个数字需要操作n个数字（一行所有元素都要变化）
所以：
{% raw%}$$\sum\limits_{i = 1}^{n - 1} {i = {{n(n - 1)} \over 2}} $${% endraw %}


⑤ 作用：求解线性方程组：


第一步：{% raw%}$A = LU${% endraw %}


第二步：LUx=b,Ly=b先求出y，此时复杂度为{% raw%}$T = O({{{n^2}} \over 2})${% endraw %};


（因为下三角且为1每行只需要操作1所在元素，其他全为0）；

第三步：再次求解Ux=y，利用转置（或者约旦消元）可以求得
{% raw%}

$T = O({{{n^2}}})$

{% endraw %}


所以：
{% raw%}

$T = O({{{n^3}} \over 2}) + 2O({{{n^2}} \over 2})$

{% endraw %}



⑥ 与求解逆矩阵求解AX=b的对比：

求解逆矩阵：利用初等变换：为：

{% raw%}

$$T = O\{ [2n \times (n - 1) +  \cdots 2n \times 1] \times 2\}  = O(2{n^3})$$

{% endraw %}

所以LU分解性能极大增强；


<h1 align = "center">向量空间</h1>

1、空间=一个集合；

2、特殊的向量空间：

欧几里得空间{% raw%}

${R^N}$

{% endraw %}：有序实数元祖的集合（点集集合），n维空间的点；


如 {% raw%}

$\left( {\begin{array}{*{20}{c}}
5\\
{66}
\end{array}} \right)$

{% endraw %}

3、向量空间：
3.1  直观理解：元素为"向量"的集合(空间)；

3.2 什么是向量：不只是欧几里得中的有向点，还包括其他；

3.3  向量定义：

对于向量，我们定义两种运算：
> 加法与数量乘法：

同时还需要满足十条性质：

① `封闭性`：ku+v属于向量空间V（满足加法和数乘封闭）；

//例如整数对于加法封闭，除法不封闭；

② 交换律、结合律、存在0向量属于向量空间；

③ 其他可由以上两条推出！

3.4 什么是向量空间：同3.3：

欧几里得空间+其他空间；

广义向量空间：
{所有2\*2方阵，所有m\*n矩阵}；

>   所有多项式构成向量空间；所有某类函数构成向量空间}；

反例:

//所有这种矩阵
 {% raw%}

$$\left( {\matrix{
   1 & a  \cr 
   0 & 1  \cr 

 } } \right)$$

{% endraw %}
 不组成向量空间；

4、维度：
一个空间的极大无关组（基）的向量的个数；

-->>元素有几个数字，个数就是维度（错误:应当是极大无关组,注意区分向量维度和空间维度）；

`特别的三维空间的过原点的平面，定义其维度为2；`

5、行列视角：
 {% raw%}

$$\left( {\matrix{
   {{a_{11}}} &  \ldots  & {{a_{1n}}}  \cr 
    \vdots  &  \ddots  &  \vdots   \cr 
   {{a_{m1}}} &  \cdots  & {{a_{mn}}}  \cr 

 } } \right)$$

{% endraw %}

① 行空间为n维空间的子集；列空间为m维空间的子集；

② 求法：高斯行变换为最简形的非0行数=行秩=行空间的维度；

③ 秩为矩阵的秩，维度为向量空间的维度；

④ 同理列空间的极大无关组一般通过初等行变换得到RREF型求解；

>   列空间的基(极大无关组)为化简后对应的原来的矩阵的列；

⑤ 区分向量空间的维度和向量的维度；

⑥ Ra行=Rb列；所以行空间与列空间的维度相等；

6、零空间：

① 定义：齐次线性方程组的所有解向量形成一个向量空间，称为0空间；

矩阵A的零空间就是AX=0中所有x组成的空间；

② 深入理解0空间：

>   view1：A=函数变换：所有向量在矩阵A变换下映射到零点；

> view2：A=系统：Ax=0所有解组成的空间；

> view3：A=空间：零空间是一个集合，
>>其中所有向量与A行向量点乘为0,A的0空间正交于A的行空间；

③ 空间正交：
任意两个向量垂直；如直线垂直平面，但是平面垂直平面不行（公共线不垂直）

---两个二维平面（空间）在三维空间不正交，在4维正交；

④ 0空间的维度？
{% raw%}
$n - r(A)$
{% endraw %}
:原因：非自由列有{% raw%}
$r(A)$
{% endraw %}个，剩下的为自由列，也就是解的列空间个数；

秩+零化度（0空间的维度）=n

7、子空间：

① 定义：向量空间V的子集S并且`要求S还是一个向量空间`；

或者：S为V空间：S为向量空间V子集同时对加法、数乘封闭；

如平面xoy上过原点的直线（不能是射线）为其子空间，不过原点就不是子空间（不包含0向量）；

② 举例：

如三维空间，原点，过原点直线，过原点平面都是子空间；

\--推而广之n为空间；

③ 子空间关系：

列空间：
{% raw%}

$$col(A)\;\;\;\;\;\;\;\;\;\;r = a;$$

{% endraw %}
行空间：
{% raw%}

$$col({A^T})\;\;\;\;\;\;\;\;\;\;r = a;$$

{% endraw %}
右0空间：
{% raw%}
$$null(A)\;\;\;\;\;\;r = n - r(A)$$
{% endraw %}
//正交于行空间；

左0空间：
{% raw%}
$$null(A)\;\;\;\;\;\;r = n - r(A^T)$$
{% endraw %}
//正交于列空间；

④ 子空间作用：  `降维`：

> AX=b，方程组个数太多（采集很多样本）很容易无解；
>>于是对于Ax（表示矩阵A的列（向量组成的）空间）再取求解离A列空间离b最近的b’,求解Ax=b';

//这就是`最小二乘法`的思路！



<h1 align = "center">施密特正交化</h1>

1、二维情形：

问题：

已知u，v求垂直于u的向量

解法：

v垂直于u的投影向量可以求出为p，那么v-p向量必然垂直于u

所以：
{% raw%}
$$\overrightarrow u ,\overrightarrow v  - {{\overrightarrow u  \cdot \overrightarrow v } \over {\left\| {\overrightarrow u } \right\|}} \cdot {{\overrightarrow u } \over {\left\| {\overrightarrow u } \right\|}}$$

{% endraw %}

就是所求的垂直向量；

2、三维：

a,b,c：任选两个a,b--u,v施法正交，第三个c投影到二维平面(uov)求第三个基，为方便求解，根据立体几何知识只需要在c投影到u,v二轴，求出投影向量k，然后c-k=w即可；

<h1 align = "center">分解</h1>

1、矩阵QR分解：
{% raw%}

$A = QR$

{% endraw %}
① 含义：Q:标准正交矩阵；R（上三角矩阵）

② 用途：解方程组：
{% raw%}
$Ax = b,QRx = b,Rx = {Q^T}b$
{% endraw %}
因为右边好求，R为三角阵也好求，所以简化线性方程组求解办法；

③ 如何求解Q,R?

方法1：求法：借助`施法正交`等式，求出
{% raw%}
${a_i}，{b_i}$
{% endraw %}的关系；

{% raw%}
$$A = \left( {{a_1},{a_2}, \ldots ,{a_n}} \right)$$
{% endraw %}施法正交为(p1,p2,pn),规范化为(q1,q2,qn)

那么：
{% raw%}

$\left\| {{p_1}} \right\|\overrightarrow {{q_1}}  = \overrightarrow {{p_1}}  = {a_1} \Rightarrow {a_1} = {r_{11}}{p_1}$

{% endraw %}

{% raw%}

$\left\| {{p_2}} \right\|\overrightarrow {{q_2}}  = \overrightarrow {{p_2}}  = {a_2} - {{{a_2} \cdot {p_1}} \over {{p_1}^2}} \cdot {p_1} \Rightarrow {a_2} = {r_{21}}{q_1} + {r_{22}}{q_2}$

{% endraw %}
所以：
{% raw%}
$A = Q \cdot \left( {\matrix{
   {{r_{11}}} & {{r_{21}}} & {{r_{31}}}  \cr 
   0 & {{r_{22}}} & {{r_{32}}}  \cr 
   0 & 0 & {{r_{33}}}  \cr 
 } } \right)$
{% endraw %}
方法2：实际使用的最多还是：
{% raw%}

$A{\rm{ = }}Q \cdot R$

{% endraw %}
{% raw%}
$$R = {Q^T} \cdot A$$


{% endraw %}



<h1 align = "center">基、坐标变换</h1>


1、点的坐标以一组基为标准，
 {% raw%}
$\left( {\matrix{
   x  \cr 
   y  \cr 

 } } \right) = \left( {\matrix{
   1 & 0  \cr 
   0 & 1  \cr 

 } } \right) \cdot \left( {\matrix{
   x  \cr 
   y  \cr 

 } } \right) = x \cdot \left( {\matrix{
   1  \cr 
   0  \cr 

 } } \right) + y \cdot \left( {\matrix{
   0  \cr 
   1  \cr 

 } } \right)$


{% endraw %}

坐标的值为对应基的线性组合的系数；

2、标准基:

含义：
{% raw%}

$\left( {{e_1},{e_2},{e_3} \cdots {e_n}} \right)$

{% endraw %}
即:标准坐标系；

3、标准正交基和标准基；

前者为单位正交向量组，后者特指标准坐标系的基向量；

4、坐标系转换：

桥梁：{% raw%}

${P_a} \cdot {x_a} = {P_b} \cdot {x_b}$

{% endraw %}，
{% raw%}

${P_b}^{ - 1} \cdot {P_a} \cdot {x_a} = {x_b} >  >  > {P_{a -  > b}} \cdot {x_a} = {x_b}$

{% endraw %}（直观理解：条条大路走到该点）；

举例：
{% raw%}
$$\left( {\matrix{
   1 & 0  \cr 
   0 & 1  \cr 

 } } \right) \cdot \left( {\matrix{
   3  \cr 
   9  \cr 

 } } \right) = \left( {\matrix{
   1 & 1  \cr 
   4 & 1  \cr 

 } } \right) \cdot \left( {\matrix{
   2  \cr 
   1  \cr 

 } } \right) = \left( {\matrix{
   3  \cr 
   9  \cr 

 } } \right) = \overrightarrow v $$


{% endraw %}





<h1 align = "center">线性变换</h1>

1、线性变换：{% raw%}

$T(v)$

{% endraw %}
条件：
{% raw%}

$$T(v + u) = T(v) + T(u);T(cv) = cT(v)$$
{% endraw %}
> 特别的：在欧几里得空间中，矩阵=线性变换；

>   此时：
{% raw%}

$$T(\overrightarrow v ) = A \cdot \overrightarrow {(v)} $$

{% endraw %}
2、不同维度空间的变化：
3D到2D动画；计算机视觉：2D->3D；

同维度：数据压缩---新的基中y方向差别很小，降维为x一维，

>如jpeg、傅里叶变换都是找新基，得到新特征；

<h1 align = "center">行列式</h1>

1、行列式：方阵的一个属性；

行列式表示向量组在空间的有向体积！

2、行列式电算：

> **高斯消元法化为上三角（注意不能倍乘、交换），主对角线出现0就是0**；

<h1 align = "center">特征值、特征向量</h1>


1、特征值、特征向量：方阵的一个属性；

① 定义：
{% raw%}

$$A\varepsilon {\rm{ = }}\lambda \varepsilon $
(%endraw%)
(%raw%)$\varepsilon  \ne \overrightarrow 0 $$

{% endraw %}
② 特征空间即：{% raw%}

$\{ \overrightarrow 0 \} $

{% endraw %}∪{λ的特征向量}，即A-λI的零空间；

2、投影变换：

① 投影变换为线性变换，对应一个矩阵，即：

> **矩阵=投影变换=线性变换；**

特征向量：经过投影变换后的向量与原来的向量在同一直线的向量；

如
{% raw%}

$$A = \left( {\matrix{
   0 & 1  \cr 
   1 & 0  \cr 

 } } \right)$$

{% endraw %}
表示翻转变化，关于y=x对称，此时特征向量为y=x上向量，对应特征值为1；

而y=-x上向量翻转后对应λ为-1的特征向量；

② 复数：
{% raw%}

$$A{\rm{ = }}\left( {\matrix{
   0 & {{\rm{ - }}1}  \cr 
   1 & 0  \cr 

 } } \right)$$

{% endraw %}
基坐标来看是旋转，直观理解没有特征向量，更不用说特征值，求出λ为复数；

③ 任意：

单位矩阵：A=I，特征值为1，特征向量为任何向量，基为
{% raw%}

$$\left( {{e_i},{e_j}} \right)$$

{% endraw %}
④ 代数重数大于等于几何重数（特征空间的维度）；

3、矩阵相似：
{% raw%}
$$A = {P^{ - 1}}BP$$


{% endraw %}
① 几何解释：P是一个坐标系，A变换是P坐标系下观察的B变换；

② 类似于相似三角形；

③ A和B本质是一个变换，只不过观察的坐标系不同；

④ 特例：{% raw%}

$A = {P^{ - 1}} \wedge P$

{% endraw %}表示A变换在P坐标系下观察到的变换^；

//应用：求方阵的幂；

⑤ 为什么求矩阵的幂？

动态系统：^中的特征值反应系统各个分量的变化速率；

<h1 align = "center">对称阵、奇异值</h1>

1、对称阵：
{% raw%}

$${A^T} = A$$

{% endraw %}
① 特征值一定是实数！并且一定可以相似对角化--几何重数一定等于代数重数；

② 不同特征值对应特征向量正交；

③ 对称阵一定可以正交变换为对角阵；

2、奇异值：
如果有{% raw%}

${A_{m \times n}}$

{% endraw %}，那么{% raw%}

${A^T} \cdot A{\rm{ = }}C$

{% endraw %}为对称方阵，可正交对角化

有
{% raw%}
$${\lambda _i} \to {\varepsilon _i}$$


{% endraw %}（已规范化）
其中{% raw%}

${C_{ij}}$

{% endraw %}为A的i列乘以A的j列；

那么：
{% raw%}

$${\left\| {A\overrightarrow {{\varepsilon _i}} } \right\|^2} = {(A\overrightarrow {{\varepsilon _i}} )^T}(A\overrightarrow {{\varepsilon _i}} ) = {(\overrightarrow {{\varepsilon _i}} )^T}{\lambda _i}{\varepsilon _i} = {\lambda _i}$$

{% endraw %}

① {% raw%}

${\lambda _i} \ge 0$

{% endraw %}，{% raw%}
${\sigma _i} = \sqrt {{\lambda _i}} $
${\lambda _i} \ge 0$
表示矩阵的奇异值；

{% endraw %}
② 奇异值表示即向量{% raw%}
$(A\overrightarrow {{\varepsilon _i}} )$


{% endraw %}的长度；

③ {% raw%}

${\rm{\{ }}A\overrightarrow {{\varepsilon _1}} {\rm{,}}A\overrightarrow {{\varepsilon _2}} ,A\overrightarrow {{\varepsilon _3}} {\rm{,}} \cdots {\rm{,}}A\overrightarrow {{\varepsilon _r}} {\rm{\} }}\;\;(\lambda i \ne 0)$

{% endraw %}表示A的列空间的一组正交基，；

④ 注意事项：奇异值是对于矩阵{% raw%}
$ A$


{% endraw %}而言，特征值是对于{% raw%}
${A^T} \cdot A$


{% endraw %}而言，

A的奇异值为{% raw%}
${A^T} \cdot A$


{% endraw %}的特征值开根号！

⑤ 如果A有个{% raw%}
$r$


{% endraw %}个{% raw%}

${\lambda _i}$

{% endraw %}不等于0，

那么{% raw%}

$${\rm{\{ }}A\overrightarrow {{\varepsilon _1}} {\rm{,}}A\overrightarrow {{\varepsilon _2}} ,A\overrightarrow {{\varepsilon _3}} {\rm{,}} \cdots {\rm{,}}A\overrightarrow {{\varepsilon _r}} {\rm{\} }}\;\;(\lambda i \ne 0)$$

{% endraw %}组成A的列空间的一组正交基，
且：{% raw%}
${Q^T}{A^T}AQ =  \wedge ,R({A^T}A) = R( \wedge ) = r = R(A)$


{% endraw %}

那么：{% raw%}

$${\rm{\{ }}{{A\overrightarrow {{\varepsilon _i}} } \over {{\sigma _i}}}{\rm{\} }}\;\;(\lambda i \ne 0)$$

{% endraw %}表示的列空间的一组标准正交基；

3、SVD分解：对矩阵A没有限制

① 矩阵的奇异值分解（不是矩阵的分解）：

② 形式：
{% raw%}


$${A_{m \times n}} = {U_{m \times m}}{\Sigma _{m \times n}}{V^T}_{n \times n}$$
{% endraw %}
![](https://cdn.jsdelivr.net/gh/Zhujie-ww/imagebed/imagebed/正交.png)



{% raw%}
${{\rm{U}}_{m \times m}}${% endraw %}为标准正交阵；

{% raw%}

${V^T}$

{% endraw %}是{% raw%}

${A^T}A$

{% endraw %}的特征向量进行标准化之后的矩阵（标准正交矩阵）；

Σ：奇异值矩阵：
{% raw%}

$$\Sigma {\rm{ = }}\left( {\begin{array}{*{20}{c}}
{{\sigma _1}}&0&0\\
0&{{\sigma _r}}&0\\
0&0&0
\end{array}} \right)$$

{% endraw %}
其中：{% raw%}

${\sigma _i} \ne 0,{\sigma _i} \ge {\sigma _{i + 1}}$

{% endraw %}：
③ 证明
{% raw%}

$A = U\Sigma {V^T} \leftarrow AV = U\Sigma  \leftarrow AV = (A\overrightarrow {{v_i}} ),U\Sigma  = (A\overrightarrow {{v_i}} ,0)$

{% endraw %}
④ 算法过程：

S1:求解{% raw%}

${A^T}A$

{% endraw %}特征值、特征向量；

S2:奇异值从大到小排序，得到Σ；

S3:特征向量标准化得到矩阵
{% raw%}

$V$

{% endraw %}；
S4:求出{% raw%}

${u_i} = \frac{{A\overrightarrow {{\varepsilon _i}} }}{{{\sigma _i}}}$

{% endraw %}{% raw%}
${\lambda _i}≠0$


{% endraw %}，然后施法拓展；

⑤ SVD分解应用：

应用1： 对A一个N维列向量变换：
{% raw%}
$Ax = U\Sigma {V^T}x = U\Sigma {V^T}Vy = U(\Sigma y) = U\left( {\begin{array}{*{20}{c}}
{{\sigma _1}{y_1}}\\
{\begin{array}{*{20}{c}}
{{\sigma _2}{y_2}}\\
{{\sigma _r}{y_r}}\\
0
\end{array}}
\end{array}} \right) = U({\sigma _i}{y_i})$


{% endraw %}
几何意义：原来的{% raw%}
$x$
{% endraw %}在基坐标下坐标为{% raw%}

${y_i}$

{% endraw %}，经过A变换后，在{% raw%}

$U$

{% endraw %}坐标系下为对{% raw%}
${y_i}$


{% endraw %}进行奇异值倍拉伸；

举例：圆—变换为-椭圆；

应用2：{% raw%}

$A = (U\Sigma ){V^T} = ({u_1}{\sigma _1}, \cdots ,{u_r}{\sigma _r},0){V^T} = \sum\limits_1^r {{u_i}{\sigma _i}{v_i}^T} ，{\sigma _i} \ge {\sigma _{i{\rm{ + }}1}}$

{% endraw %}
几何意义：矩阵可以看成多个递降权值的矩阵；

//因此可以进行（图像）矩阵压缩、降噪、降维，保留大的奇异值的矩阵；

应用3： 应用于推荐系统、NLP、搜索引擎；

4、最后展望；

1、统计学：PCA，最小二乘法；

微积分：差分方程、微分方程；

2、机器学习、推荐系统、图像学、经济学；

3、对领域感兴趣去学习，再补数学，有目的；
